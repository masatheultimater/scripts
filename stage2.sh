#!/bin/bash
# ============================================================
# STAGE 2: Claude Code ã«ã‚ˆã‚‹ãƒãƒ¼ãƒˆç”Ÿæˆ
# ä½¿ã„æ–¹: bash stage2.sh [--dry-run] <SAFE_NAME> [SOURCE_TYPE]
# ingest.sh ã‹ã‚‰å‘¼ã°ã‚Œã‚‹ã€‚å˜ç‹¬å®Ÿè¡Œã‚‚å¯ï¼ˆSTAGE 1 ã®ãƒªã‚«ãƒãƒªç”¨ï¼‰
# ============================================================

set -euo pipefail

VAULT="$HOME/vault/houjinzei"
LOG_DIR="$VAULT/logs"
RUN_TS="$(date +%Y%m%d_%H%M%S)"
LOG_FILE="$LOG_DIR/stage2_${RUN_TS}.log"

usage() {
  echo "ä½¿ã„æ–¹: bash stage2.sh [--dry-run] <SAFE_NAME> [SOURCE_TYPE]"
  echo "ä¾‹1:   bash stage2.sh è¨ˆç®—å•é¡Œé›†â‘ "
  echo "ä¾‹2:   bash stage2.sh è¨ˆç®—å•é¡Œé›†â‘  è¨ˆç®—å•é¡Œé›†"
  echo "ä¾‹3:   bash stage2.sh --dry-run è¨ˆç®—å•é¡Œé›†â‘ "
  echo "ä¾‹4:   bash stage2.sh è¨ˆç®—å•é¡Œé›†â‘  --dry-run"
}

mkdir -p "$LOG_DIR"
exec > >(tee -a "$LOG_FILE") 2>&1

START_TIME="$(date '+%Y-%m-%d %H:%M:%S')"
on_exit() {
  local exit_code=$?
  local end_time
  end_time="$(date '+%Y-%m-%d %H:%M:%S')"
  echo ""
  echo "çµ‚äº†æ™‚åˆ»: $end_time"
  echo "çµ‚äº†ã‚³ãƒ¼ãƒ‰: $exit_code"
  echo "ãƒ­ã‚°: $LOG_FILE"
}
trap on_exit EXIT

DRY_RUN=false
if [ "${1:-}" = "--dry-run" ]; then
  DRY_RUN=true
  shift
fi
if [ "${2:-}" = "--dry-run" ]; then
  DRY_RUN=true
  if [ $# -ge 3 ]; then
    set -- "$1" "$3"
  else
    set -- "$1"
  fi
fi

if [ $# -lt 1 ]; then
  usage
  exit 1
fi

if [ $# -gt 2 ]; then
  usage
  exit 1
fi

SAFE_NAME="$1"
SOURCE_TYPE="${2:-}"
TOPICS_FILE="$VAULT/02_extracted/${SAFE_NAME}_topics.json"

echo "é–‹å§‹æ™‚åˆ»: $START_TIME"
echo "å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: $TOPICS_FILE"
if [ -n "$SOURCE_TYPE" ]; then
  echo "æ•™æã‚¿ã‚¤ãƒ—: $SOURCE_TYPE"
fi
echo "ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³: $DRY_RUN"
echo ""

if [ ! -f "$TOPICS_FILE" ]; then
  echo "âŒ topics.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: $TOPICS_FILE"
  exit 1
fi

echo "ğŸ“ Claude Code ã§ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹..."
echo "   å…¥åŠ›: $TOPICS_FILE"
echo ""

SOURCE_TYPE_PROMPT=""
if [ -n "$SOURCE_TYPE" ]; then
  SOURCE_TYPE_PROMPT="
## æ•™æã‚¿ã‚¤ãƒ—ï¼ˆæŒ‡å®šã‚ã‚Šï¼‰
$SOURCE_TYPE

### æ•™æã‚¿ã‚¤ãƒ—ã®åæ˜ 
- è«–ç‚¹ãƒãƒ¼ãƒˆã®æœ¬æ–‡ï¼ˆæ¦‚è¦ãƒ»è¨ˆç®—æ‰‹é †ãƒ»åˆ¤æ–­ãƒã‚¤ãƒ³ãƒˆï¼‰ã®æ›¸ãã¶ã‚Šã¯ã€ã“ã®æ•™æã‚¿ã‚¤ãƒ—ã«åˆã‚ã›ã¦èª¿æ•´ã™ã‚‹"
fi

if [ "$DRY_RUN" = true ]; then
  echo "ğŸ” DRY RUN: claude -p ã¯å®Ÿè¡Œã—ã¾ã›ã‚“"
  python3 << PYEOF
import json
import os
import sys

topics_file = "$TOPICS_FILE"
vault = "$VAULT"

with open(topics_file, "r", encoding="utf-8") as f:
    data = json.load(f)

if isinstance(data, dict):
    topics = data.get("topics", [])
elif isinstance(data, list):
    topics = data
else:
    print("âŒ topics.json ã®å½¢å¼ãŒä¸æ­£ã§ã™")
    sys.exit(1)

if not isinstance(topics, list):
    print("âŒ topics.json ã® topics ãŒé…åˆ—ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
    sys.exit(1)

create_notes = []
update_notes = []
topic_ids = []

for t in topics:
    if not isinstance(t, dict):
        continue
    topic_id = str(t.get("topic_id", "")).strip()
    category = str(t.get("category", "ãã®ä»–")).strip() or "ãã®ä»–"
    if not topic_id:
        continue
    topic_ids.append(topic_id)
    note_path = os.path.join(vault, "10_è«–ç‚¹", category, f"{topic_id}.md")
    if os.path.exists(note_path):
        update_notes.append(note_path)
    else:
        create_notes.append(note_path)

print(f"å¯¾è±¡topicæ•°: {len(topic_ids)}")
print("topic_idä¸€è¦§:")
for tid in topic_ids:
    print(f"  - {tid}")

print("")
print(f"æ–°è¦ä½œæˆäºˆå®šãƒãƒ¼ãƒˆ: {len(create_notes)}")
for path in create_notes:
    print(f"  + {path}")

print("")
print(f"æ›´æ–°äºˆå®šãƒãƒ¼ãƒˆ: {len(update_notes)}")
for path in update_notes:
    print(f"  * {path}")

print("")
print("ã‚½ãƒ¼ã‚¹ãƒãƒƒãƒ—:")
print(f"  - {vault}/30_ã‚½ãƒ¼ã‚¹åˆ¥/ é…ä¸‹ã‚’ClaudeãŒ source_name ã«åŸºã¥ãç”Ÿæˆ/æ›´æ–°")
PYEOF

  echo ""
  echo "âœ… STAGE 2 DRY RUN å®Œäº†"
  exit 0
fi

python3 << PYEOF
import datetime
import json
import os
import re
import sys

topics_file = "$TOPICS_FILE"
vault = "$VAULT"
source_type_prompt = """$SOURCE_TYPE_PROMPT"""

def ensure_list(value):
    if isinstance(value, list):
        return value
    if value in (None, ""):
        return []
    return [value]

def yaml_inline(value):
    return json.dumps(value, ensure_ascii=False)

def sanitize_path_name(name: str) -> str:
    return re.sub(r'[\\\\/:*?"<>|]+', "_", str(name)).strip() or "unknown"

def append_source_frontmatter_only(note_path: str, source_entry: str) -> bool:
    with open(note_path, "r", encoding="utf-8") as f:
        content = f.read()

    if not content.startswith("---\n"):
        return False

    lines = content.splitlines(keepends=True)
    end_idx = None
    for i in range(1, len(lines)):
        if lines[i].strip() == "---":
            end_idx = i
            break
    if end_idx is None:
        return False

    fm_lines = lines[1:end_idx]
    body_lines = lines[end_idx + 1:]

    source_line = f"  - '{source_entry}'\n"
    src_idx = None
    for i, line in enumerate(fm_lines):
        if line.strip() == "sources:":
            src_idx = i
            break

    changed = False
    if src_idx is None:
        insert_idx = len(fm_lines)
        for i, line in enumerate(fm_lines):
            if re.match(r"^(keywords|related|kome_total|calc_correct|calc_wrong|last_practiced|stage|status|pdf_refs|mistakes|extracted_from):", line.strip()):
                insert_idx = i
                break
        fm_lines[insert_idx:insert_idx] = ["sources:\n", source_line]
        changed = True
    else:
        j = src_idx + 1
        existing_sources = []
        while j < len(fm_lines):
            raw = fm_lines[j]
            stripped = raw.strip()
            if stripped.startswith("- "):
                item = stripped[2:].strip().strip("'\"")
                existing_sources.append(item)
                j += 1
                continue
            if stripped == "":
                j += 1
                continue
            break
        if source_entry not in existing_sources:
            fm_lines.insert(j, source_line)
            changed = True

    if changed:
        new_content = "".join(["---\n"] + fm_lines + ["---\n"] + body_lines)
        with open(note_path, "w", encoding="utf-8") as f:
            f.write(new_content)
    return changed

with open(topics_file, "r", encoding="utf-8") as f:
    data = json.load(f)

if isinstance(data, dict):
    topics = data.get("topics", [])
else:
    topics = data

if not isinstance(topics, list):
    print("âŒ topics.json ã® topics ãŒé…åˆ—ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
    sys.exit(1)

publisher = ""
source_name = ""
source_type = ""
total_problems = 0
short_source_name = ""

if isinstance(data, dict):
    publisher = str(data.get("publisher", "")).strip()
    source_name = str(data.get("source_name", "")).strip()
    source_type = str(data.get("source_type", "")).strip()
    total_problems = data.get("total_problems", 0)
    short_source_name = str(
        data.get("short_source_name", "") or data.get("source_short_name", "") or source_name
    ).strip()

if not source_type and source_type_prompt.strip():
    for line in source_type_prompt.splitlines():
        s = line.strip()
        if s and not s.startswith("#") and not s.startswith("-"):
            source_type = s
            break

publisher_folder = sanitize_path_name(publisher or "ä¸æ˜å‡ºç‰ˆç¤¾")
safe_source_name = sanitize_path_name(source_name or "unknown_source")
source_entry = f"{publisher} {short_source_name}".strip()
today = datetime.date.today().isoformat()

created_count = 0
updated_count = 0
mapping_rows = []

for topic in topics:
    if not isinstance(topic, dict):
        continue

    topic_id = str(topic.get("topic_id", "")).strip()
    if not topic_id:
        continue

    topic_name = str(topic.get("name", topic_id)).strip() or topic_id
    category = str(topic.get("category", "ãã®ä»–")).strip() or "ãã®ä»–"
    subcategory = str(topic.get("subcategory", "")).strip()
    topic_type = ensure_list(topic.get("type", []))
    importance = str(topic.get("importance", "")).strip()
    conditions = ensure_list(topic.get("conditions", []))
    keywords = ensure_list(topic.get("keywords", []))
    related = ensure_list(topic.get("related", []))
    problem_numbers = ensure_list(topic.get("problem_numbers", []))

    safe_topic_id = topic_id.replace("/", "_")
    category_dir = os.path.join(vault, "10_è«–ç‚¹", category)
    os.makedirs(category_dir, exist_ok=True)
    note_path = os.path.join(category_dir, f"{safe_topic_id}.md")

    if os.path.exists(note_path):
        if append_source_frontmatter_only(note_path, source_entry):
            updated_count += 1
    else:
        frontmatter_lines = [
            "---",
            f"topic: {topic_id}",
            f"category: {category}",
            f"subcategory: {subcategory}",
            f"type: {yaml_inline(topic_type)}",
            f"importance: {importance}",
            f"conditions: {yaml_inline(conditions)}",
            "sources:",
            f"  - '{source_entry}'",
            f"keywords: {yaml_inline(keywords)}",
            f"related: {yaml_inline(related)}",
            "kome_total: 0",
            "calc_correct: 0",
            "calc_wrong: 0",
            "last_practiced:",
            "stage: æœªç€æ‰‹",
            "status: æœªç€æ‰‹",
            "pdf_refs: []",
            "mistakes: []",
            f"extracted_from: 'Gemini {today}'",
            "---",
            "",
        ]
        body_lines = [
            f"# {topic_name}",
            "## æ¦‚è¦",
            "## è¨ˆç®—æ‰‹é †",
            "## åˆ¤æ–­ãƒã‚¤ãƒ³ãƒˆ",
            "## é–“é•ãˆã‚„ã™ã„ãƒã‚¤ãƒ³ãƒˆ",
            "## é–¢é€£æ¡æ–‡",
            "",
        ]
        content = "\n".join(frontmatter_lines + body_lines)
        with open(note_path, "w", encoding="utf-8") as f:
            f.write(content)
        created_count += 1

    if problem_numbers:
        for p in problem_numbers:
            mapping_rows.append((str(p).strip(), topic_name, topic_id))
    else:
        mapping_rows.append(("", topic_name, topic_id))

source_map_dir = os.path.join(vault, "30_ã‚½ãƒ¼ã‚¹åˆ¥", publisher_folder)
os.makedirs(source_map_dir, exist_ok=True)
source_map_path = os.path.join(source_map_dir, f"{safe_source_name}.md")

header_lines = [
    "---",
    f"source_name: {source_name}",
    f"source_type: {source_type}",
    f"publisher: {publisher}",
    f"total_problems: {total_problems}",
    "covered: 0",
    "---",
    "",
    f"# {source_name or safe_source_name}",
    "",
    "## å•é¡Œ â†’ è«–ç‚¹ ãƒãƒƒãƒ”ãƒ³ã‚°",
    "",
    "| å•é¡Œç•ªå· | è«–ç‚¹ | ãƒªãƒ³ã‚¯ |",
    "|---|---|---|",
]

row_lines = [
    f"| {problem_no} | {topic_name} | [[{topic_id}]] |"
    for problem_no, topic_name, topic_id in mapping_rows
]
source_map_content = "\n".join(header_lines + row_lines + [""])
with open(source_map_path, "w", encoding="utf-8") as f:
    f.write(source_map_content)

print(f"ä½œæˆãƒãƒ¼ãƒˆæ•°: {created_count}")
print(f"æ›´æ–°ãƒãƒ¼ãƒˆæ•°: {updated_count}")
print(f"ã‚½ãƒ¼ã‚¹ãƒãƒƒãƒ—: {source_map_path}")
PYEOF

echo ""
echo "âœ… STAGE 2 å®Œäº†"
